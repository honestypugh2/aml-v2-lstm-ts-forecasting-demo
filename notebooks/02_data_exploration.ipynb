{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc93309",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing\n",
    "\n",
    "This notebook covers:\n",
    "1. Loading and exploring time series data\n",
    "2. Data quality assessment\n",
    "3. Feature engineering for LSTM models\n",
    "4. Data preprocessing and transformation\n",
    "5. Train/validation/test split strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffc16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import our preprocessing utilities\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "from data_processing.preprocessor import TimeSeriesPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1bdd0f",
   "metadata": {},
   "source": [
    "## 1. Load Sample Time Series Data\n",
    "\n",
    "For this example, we'll create synthetic time series data. In a real project, you would load your actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time series data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')\n",
    "n_points = len(dates)\n",
    "\n",
    "# Create trend + seasonality + noise\n",
    "trend = np.linspace(100, 150, n_points)\n",
    "seasonal = 10 * np.sin(2 * np.pi * np.arange(n_points) / 365.25)  # Yearly seasonality\n",
    "weekly = 5 * np.sin(2 * np.pi * np.arange(n_points) / 7)  # Weekly seasonality\n",
    "noise = np.random.normal(0, 5, n_points)\n",
    "\n",
    "values = trend + seasonal + weekly + noise\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'value': values,\n",
    "    'feature_1': np.random.normal(50, 10, n_points),\n",
    "    'feature_2': np.random.exponential(2, n_points)\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad80e3",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ba15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c046a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Main time series\n",
    "axes[0, 0].plot(df['date'], df['value'])\n",
    "axes[0, 0].set_title('Time Series Data')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "\n",
    "# Distribution\n",
    "axes[0, 1].hist(df['value'], bins=50, alpha=0.7)\n",
    "axes[0, 1].set_title('Value Distribution')\n",
    "axes[0, 1].set_xlabel('Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Seasonal decomposition (simplified)\n",
    "monthly_avg = df.set_index('date')['value'].resample('M').mean()\n",
    "axes[1, 0].plot(monthly_avg.index, monthly_avg.values)\n",
    "axes[1, 0].set_title('Monthly Average')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Monthly Avg Value')\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = df[['value', 'feature_1', 'feature_2']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Feature Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88dac2",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ab7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Check date continuity\n",
    "date_diff = df['date'].diff().dropna()\n",
    "expected_freq = pd.Timedelta(days=1)\n",
    "irregular_dates = date_diff[date_diff != expected_freq]\n",
    "print(f\"\\nIrregular date intervals: {len(irregular_dates)}\")\n",
    "\n",
    "# Outlier detection (simple method)\n",
    "Q1 = df['value'].quantile(0.25)\n",
    "Q3 = df['value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['value'] < lower_bound) | (df['value'] > upper_bound)]\n",
    "print(f\"\\nOutliers detected: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e5960",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909732e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    target_column='value',\n",
    "    sequence_length=30,\n",
    ")\n",
    "\n",
    "# Create time-based features\n",
    "df_features = df.copy()\n",
    "df_features['year'] = df_features['date'].dt.year\n",
    "df_features['month'] = df_features['date'].dt.month\n",
    "df_features['day_of_week'] = df_features['date'].dt.dayofweek\n",
    "df_features['day_of_year'] = df_features['date'].dt.dayofyear\n",
    "df_features['quarter'] = df_features['date'].dt.quarter\n",
    "df_features['is_weekend'] = (df_features['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Lag features\n",
    "for lag in [1, 7, 30]:\n",
    "    df_features[f'value_lag_{lag}'] = df_features['value'].shift(lag)\n",
    "\n",
    "# Rolling statistics\n",
    "for window in [7, 30]:\n",
    "    df_features[f'value_rolling_mean_{window}'] = df_features['value'].rolling(window=window).mean()\n",
    "    df_features[f'value_rolling_std_{window}'] = df_features['value'].rolling(window=window).std()\n",
    "\n",
    "print(f\"Features created. New shape: {df_features.shape}\")\n",
    "print(\"\\nNew features:\")\n",
    "new_features = [col for col in df_features.columns if col not in df.columns]\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf26b5a",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values (due to lag and rolling features)\n",
    "df_clean = df_features.dropna().reset_index(drop=True)\n",
    "print(f\"Clean dataset shape: {df_clean.shape}\")\n",
    "\n",
    "# Fit the preprocessor and get scaled target values\n",
    "scaled_values = preprocessor.fit_transform(df_clean)\n",
    "print(f\"Scaled values shape: {scaled_values.shape}\")\n",
    "\n",
    "# Create a DataFrame with only numeric features for sequence creation\n",
    "# Exclude date column and keep only numeric features\n",
    "numeric_features = [col for col in df_clean.columns if col != 'date' and pd.api.types.is_numeric_dtype(df_clean[col])]\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "\n",
    "# Create feature matrix (excluding date)\n",
    "feature_matrix = df_clean[numeric_features].values\n",
    "print(f\"Feature matrix shape: {feature_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614f371",
   "metadata": {},
   "source": [
    "## 6. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3279ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split (chronological order is important for time series)\n",
    "total_samples = len(df_clean)\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.2 * total_samples)\n",
    "\n",
    "train_data = df_clean[:train_size]\n",
    "val_data = df_clean[train_size:train_size + val_size]\n",
    "test_data = df_clean[train_size + val_size:]\n",
    "\n",
    "print(f\"Train set: {len(train_data)} samples ({len(train_data)/total_samples*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(val_data)} samples ({len(val_data)/total_samples*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_data)} samples ({len(test_data)/total_samples*100:.1f}%)\")\n",
    "\n",
    "# Visualize the split\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(train_data['date'], train_data['value'], label='Train', alpha=0.8)\n",
    "plt.plot(val_data['date'], val_data['value'], label='Validation', alpha=0.8)\n",
    "plt.plot(test_data['date'], test_data['value'], label='Test', alpha=0.8)\n",
    "plt.axvline(x=train_data['date'].iloc[-1], color='red', linestyle='--', alpha=0.7, label='Train/Val Split')\n",
    "plt.axvline(x=val_data['date'].iloc[-1], color='orange', linestyle='--', alpha=0.7, label='Val/Test Split')\n",
    "plt.title('Train/Validation/Test Split')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2e829",
   "metadata": {},
   "source": [
    "## 7. Create LSTM Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5eb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split feature matrix for train/val/test\n",
    "total_samples = len(feature_matrix)\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.2 * total_samples)\n",
    "\n",
    "train_features = feature_matrix[:train_size]\n",
    "val_features = feature_matrix[train_size:train_size + val_size]\n",
    "test_features = feature_matrix[train_size + val_size:]\n",
    "\n",
    "# Create sequences for LSTM training using feature matrices\n",
    "X_train, y_train = preprocessor.create_sequences(train_features)\n",
    "X_val, y_val = preprocessor.create_sequences(val_features)\n",
    "X_test, y_test = preprocessor.create_sequences(test_features)\n",
    "\n",
    "print(f\"Training sequences: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation sequences: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test sequences: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# Visualize a few sequences (using the first feature which is the target 'value')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(4):\n",
    "    # Plot the input sequence (first feature = target variable)\n",
    "    axes[i].plot(X_train[i, :, 0], label='Input Sequence', marker='o', markersize=3)\n",
    "    # Plot the target value (first feature of the target) - extract the scalar value\n",
    "    target_value = y_train[i, 0, 0] if y_train.ndim == 3 else y_train[i, 0]\n",
    "    axes[i].axhline(y=target_value, color='red', linestyle='--', label='Target')\n",
    "    axes[i].set_title(f'Training Sequence {i+1}')\n",
    "    axes[i].set_xlabel('Time Step')\n",
    "    axes[i].set_ylabel('Scaled Value')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display information about the sequences\n",
    "print(\"\\nSequence shapes:\")\n",
    "print(f\"X_train sample shape: {X_train[0].shape} (30 timesteps, 16 features)\")\n",
    "print(f\"y_train sample shape: {y_train[0].shape} (1 timestep, 16 features)\")\n",
    "print(\"\\nFirst sequence target values (all features):\")\n",
    "print(y_train[0, 0])\n",
    "print(f\"\\nTarget value (first feature only): {y_train[0, 0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bfe18e",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = Path('../data/processed')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save processed datasets\n",
    "train_data.to_csv(data_dir / 'train_data.csv', index=False)\n",
    "val_data.to_csv(data_dir / 'val_data.csv', index=False)\n",
    "test_data.to_csv(data_dir / 'test_data.csv', index=False)\n",
    "\n",
    "# Save sequences as numpy arrays\n",
    "np.save(data_dir / 'X_train.npy', X_train)\n",
    "np.save(data_dir / 'y_train.npy', y_train)\n",
    "np.save(data_dir / 'X_val.npy', X_val)\n",
    "np.save(data_dir / 'y_val.npy', y_val)\n",
    "np.save(data_dir / 'X_test.npy', X_test)\n",
    "np.save(data_dir / 'y_test.npy', y_test)\n",
    "\n",
    "print(\"Processed data saved successfully!\")\n",
    "print(f\"Files saved to: {data_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9544ce",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. ✅ Generated and explored synthetic time series data\n",
    "2. ✅ Performed data quality assessment\n",
    "3. ✅ Created time-based and lag features\n",
    "4. ✅ Preprocessed and scaled the data\n",
    "5. ✅ Split data chronologically for time series\n",
    "6. ✅ Created LSTM input sequences\n",
    "7. ✅ Saved processed data for model training\n",
    "\n",
    "**Next Steps:**\n",
    "- Use the processed data in `03_model_training.ipynb`\n",
    "- Train and evaluate LSTM models\n",
    "- Experiment with different architectures and hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_env",
   "language": "python",
   "name": "aml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
