{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f85291",
   "metadata": {},
   "source": [
    "# LSTM Model Training\n",
    "\n",
    "This notebook covers:\n",
    "1. Loading preprocessed data\n",
    "2. LSTM model architecture design\n",
    "3. Training with MLflow tracking\n",
    "4. Model evaluation and validation\n",
    "5. Hyperparameter tuning\n",
    "6. Model saving and registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our model and utilities\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "from models.lstm_model import LSTMTimeSeriesModel\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca74ce",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667550e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "data_dir = Path('../data/processed')\n",
    "\n",
    "# Load sequences\n",
    "X_train = np.load(data_dir / 'X_train.npy')\n",
    "y_train = np.load(data_dir / 'y_train.npy')\n",
    "X_val = np.load(data_dir / 'X_val.npy')\n",
    "y_val = np.load(data_dir / 'y_val.npy')\n",
    "X_test = np.load(data_dir / 'X_test.npy')\n",
    "y_test = np.load(data_dir / 'y_test.npy')\n",
    "\n",
    "print(\"Original data shapes:\")\n",
    "print(f\"Training data: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation data: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test data: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# Extract only the target variable (first feature) from y arrays\n",
    "# The target arrays have shape (n_samples, 1, n_features), we want (n_samples, 1)\n",
    "y_train = y_train[:, :, 0:1]  # Keep only first feature\n",
    "y_val = y_val[:, :, 0:1]\n",
    "y_test = y_test[:, :, 0:1]\n",
    "\n",
    "print(\"\\nAfter extracting target variable:\")\n",
    "print(f\"Training data: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation data: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test data: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "print(\"\\nTensor shapes:\")\n",
    "print(f\"X_train_tensor: {X_train_tensor.shape}\")\n",
    "print(f\"y_train_tensor: {y_train_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36636d",
   "metadata": {},
   "source": [
    "## 2. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f000340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548f668",
   "metadata": {},
   "source": [
    "## 3. Setup MLflow Tracking (Locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5462e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "\n",
    "mlflow.set_experiment(\"lstm-time-series-forecasting\")\n",
    "\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Active experiment: {mlflow.get_experiment_by_name('lstm-time-series-forecasting')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b04c25",
   "metadata": {},
   "source": [
    "## 4. Model Architecture and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb453423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train LSTM model with MLflow tracking\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_size = X_train.shape[2]  # Number of features (16)\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1  # We're predicting only 1 value (the target variable)\n",
    "dropout = 0.2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(f\"  Input size: {input_size} (number of input features)\")\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "print(f\"  Number of layers: {num_layers}\")\n",
    "print(f\"  Output size: {output_size} (predicting 1 target value)\")\n",
    "print(f\"  Dropout: {dropout}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "\n",
    "print(\"\\nData compatibility check:\")\n",
    "print(f\"  Input sequences: {X_train.shape} -> Model expects: (batch, sequence, {input_size})\")\n",
    "print(f\"  Target values: {y_train.shape} -> Model outputs: (batch, {output_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End any existing runs and start fresh\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"lstm-baseline\") as run:\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params({\n",
    "        \"input_size\": input_size,\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"output_size\": output_size,\n",
    "        \"dropout\": dropout,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size\n",
    "    })\n",
    "\n",
    "    # Initialize model\n",
    "    model = LSTMTimeSeriesModel(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        output_size=output_size,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "    # Test model with a small batch to verify shapes\n",
    "    print(\"\\nüîç Testing model forward pass...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Take first batch from train loader\n",
    "        test_batch_X, test_batch_y = next(iter(train_loader))\n",
    "        print(f\"Input batch shape: {test_batch_X.shape}\")\n",
    "        print(f\"Target batch shape: {test_batch_y.shape}\")\n",
    "\n",
    "        # Forward pass\n",
    "        test_output = model(test_batch_X)\n",
    "        print(f\"Model output shape: {test_output.shape}\")\n",
    "\n",
    "        # Check if shapes are compatible\n",
    "        if test_output.shape[0] == test_batch_y.shape[0]:\n",
    "            print(\"‚úÖ Batch sizes match!\")\n",
    "        else:\n",
    "            print(\"‚ùå Batch size mismatch!\")\n",
    "\n",
    "        print(f\"Model output sample: {test_output[0].cpu().numpy()}\")\n",
    "        print(f\"Target sample: {test_batch_y[0].cpu().numpy()}\")\n",
    "\n",
    "    # Train model (reduced epochs for testing)\n",
    "    print(f\"\\nüöÄ Starting training with {min(num_epochs, 10)} epochs...\")\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader, min(num_epochs, 10), learning_rate\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301b088",
   "metadata": {},
   "source": [
    "## 5. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe24c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses[-50:], label='Training Loss (Last 50 epochs)')\n",
    "plt.plot(val_losses[-50:], label='Validation Loss (Last 50 epochs)')\n",
    "plt.title('Training Convergence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log the plot to MLflow\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "mlflow.log_artifact('training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7b26f",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa11599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, data_name=\"Test\"):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            outputs = model(batch_X)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    print(f\"Debug - {data_name} shapes:\")\n",
    "    print(f\"  Predictions shape: {predictions.shape}\")\n",
    "    print(f\"  Actuals shape: {actuals.shape}\")\n",
    "\n",
    "    # Handle shape mismatch - predictions are (n_samples, 1) and actuals are (n_samples, 1, 1)\n",
    "    if actuals.ndim == 3 and actuals.shape[2] == 1:\n",
    "        actuals = actuals.squeeze(axis=2)  # Remove the last dimension: (n_samples, 1, 1) -> (n_samples, 1)\n",
    "\n",
    "    # Now both should be (n_samples, 1), so we can flatten safely\n",
    "    predictions_flat = predictions.flatten()\n",
    "    actuals_flat = actuals.flatten()\n",
    "\n",
    "    print(\"  After reshaping:\")\n",
    "    print(f\"    Predictions flat shape: {predictions_flat.shape}\")\n",
    "    print(f\"    Actuals flat shape: {actuals_flat.shape}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals_flat, predictions_flat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals_flat, predictions_flat)\n",
    "\n",
    "    # Calculate MAPE (Mean Absolute Percentage Error) - avoid division by zero\n",
    "    non_zero_mask = actuals_flat != 0\n",
    "    if np.any(non_zero_mask):\n",
    "        mape = np.mean(np.abs((actuals_flat[non_zero_mask] - predictions_flat[non_zero_mask]) / actuals_flat[non_zero_mask])) * 100\n",
    "    else:\n",
    "        mape = float('inf')\n",
    "\n",
    "    print(f\"\\n{data_name} Metrics:\")\n",
    "    print(f\"  MSE: {mse:.6f}\")\n",
    "    print(f\"  RMSE: {rmse:.6f}\")\n",
    "    print(f\"  MAE: {mae:.6f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'predictions': predictions,\n",
    "        'actuals': actuals\n",
    "    }\n",
    "\n",
    "# Evaluate on all datasets\n",
    "train_metrics = evaluate_model(model, train_loader, \"Training\")\n",
    "val_metrics = evaluate_model(model, val_loader, \"Validation\")\n",
    "test_metrics = evaluate_model(model, test_loader, \"Test\")\n",
    "\n",
    "# Log metrics to MLflow\n",
    "mlflow.log_metrics({\n",
    "    \"train_rmse\": train_metrics['rmse'],\n",
    "    \"train_mae\": train_metrics['mae'],\n",
    "    \"train_mape\": train_metrics['mape'],\n",
    "    \"val_rmse\": val_metrics['rmse'],\n",
    "    \"val_mae\": val_metrics['mae'],\n",
    "    \"val_mape\": val_metrics['mape'],\n",
    "    \"test_rmse\": test_metrics['rmse'],\n",
    "    \"test_mae\": test_metrics['mae'],\n",
    "    \"test_mape\": test_metrics['mape']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81085b0b",
   "metadata": {},
   "source": [
    "## 7. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4929973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Prepare data for plotting - handle potential shape differences\n",
    "test_actuals = test_metrics['actuals']\n",
    "test_predictions = test_metrics['predictions']\n",
    "\n",
    "# Ensure both are flattened for scatter plot\n",
    "if test_actuals.ndim > 1:\n",
    "    test_actuals_flat = test_actuals.flatten()\n",
    "else:\n",
    "    test_actuals_flat = test_actuals\n",
    "\n",
    "if test_predictions.ndim > 1:\n",
    "    test_predictions_flat = test_predictions.flatten()\n",
    "else:\n",
    "    test_predictions_flat = test_predictions\n",
    "\n",
    "# Test set predictions scatter plot\n",
    "axes[0, 0].scatter(test_actuals_flat, test_predictions_flat, alpha=0.6)\n",
    "axes[0, 0].plot([test_actuals_flat.min(), test_actuals_flat.max()],\n",
    "                [test_actuals_flat.min(), test_actuals_flat.max()], 'r--')\n",
    "axes[0, 0].set_xlabel('Actual Values')\n",
    "axes[0, 0].set_ylabel('Predicted Values')\n",
    "axes[0, 0].set_title('Test Set: Predictions vs Actuals')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Time series plot (first 100 test predictions)\n",
    "n_plot = min(100, len(test_predictions))\n",
    "# Ensure we get the right shape for plotting\n",
    "if test_actuals.ndim == 2:\n",
    "    actual_plot = test_actuals[:n_plot, 0] if test_actuals.shape[1] == 1 else test_actuals[:n_plot].flatten()[:n_plot]\n",
    "else:\n",
    "    actual_plot = test_actuals_flat[:n_plot]\n",
    "\n",
    "if test_predictions.ndim == 2:\n",
    "    pred_plot = test_predictions[:n_plot, 0] if test_predictions.shape[1] == 1 else test_predictions[:n_plot].flatten()[:n_plot]\n",
    "else:\n",
    "    pred_plot = test_predictions_flat[:n_plot]\n",
    "\n",
    "axes[0, 1].plot(range(n_plot), actual_plot, label='Actual', linewidth=2)\n",
    "axes[0, 1].plot(range(n_plot), pred_plot, label='Predicted', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Time Steps')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].set_title('Test Set: Time Series Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = test_actuals_flat - test_predictions_flat\n",
    "axes[1, 0].scatter(test_predictions_flat, residuals, alpha=0.6)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Predicted Values')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('Residuals Plot')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "axes[1, 1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Prediction Error')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Error Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save and log to MLflow\n",
    "plt.savefig('model_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "mlflow.log_artifact('model_evaluation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72bfd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final results summary\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Final Test Set Performance:\")\n",
    "print(f\"   RMSE: {test_metrics['rmse']:.4f}\")\n",
    "print(f\"   MAE:  {test_metrics['mae']:.4f}\")\n",
    "print(f\"   MAPE: {test_metrics['mape']:.2f}%\")\n",
    "\n",
    "print(\"\\nüìà Model Architecture:\")\n",
    "print(f\"   Input Features: {input_size}\")\n",
    "print(f\"   Hidden Units: {hidden_size}\")\n",
    "print(f\"   LSTM Layers: {num_layers}\")\n",
    "print(f\"   Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\nüíæ Model Artifacts:\")\n",
    "print(f\"   MLflow Run ID: {run.info.run_id}\")\n",
    "print(\"   Model Registry: lstm-time-series-forecasting\")\n",
    "\n",
    "print(\"\\nüîß Next Steps:\")\n",
    "print(\"   1. Deploy model using Azure ML endpoints\")\n",
    "print(\"   2. Implement automated retraining pipelines\")\n",
    "print(\"   3. Add model monitoring and drift detection\")\n",
    "print(\"   4. Experiment with advanced architectures (Transformer, GRU)\")\n",
    "\n",
    "print(\"\\nüåê MLflow UI:\")\n",
    "print(\"   Run: mlflow ui --backend-store-uri ./mlruns\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a40d4b5",
   "metadata": {},
   "source": [
    "## 8. Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5419f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_dir = Path('../outputs/models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save PyTorch model\n",
    "model_path = model_dir / 'lstm_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'input_size': input_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'output_size': output_size,\n",
    "        'dropout': dropout\n",
    "    },\n",
    "    'metrics': {\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_mape': test_metrics['mape']\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Log model to MLflow\n",
    "mlflow.pytorch.log_model(\n",
    "    pytorch_model=model,\n",
    "    artifact_path=\"lstm_model\",\n",
    "    registered_model_name=\"lstm-time-series-forecasting\"\n",
    ")\n",
    "\n",
    "print(f\"Model logged to MLflow with run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b013d",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de02bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning experiment\n",
    "hyperparams_grid = {\n",
    "    'hidden_size': [32, 64, 128],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'dropout': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter Tuning Grid:\")\n",
    "for param, values in hyperparams_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(\"\\nüí° For comprehensive hyperparameter tuning, consider using:\")\n",
    "print(\"   - Azure ML Hyperdrive for automated hyperparameter tuning\")\n",
    "print(\"   - Optuna for advanced optimization\")\n",
    "print(\"   - Ray Tune for distributed hyperparameter tuning\")\n",
    "print(\"\\nüìö Example implementation can be found in the training script:\")\n",
    "print(\"   src/training/train_lstm.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f4862",
   "metadata": {},
   "source": [
    "# Display final results summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468cfa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Final Test Set Performance:\")\n",
    "print(f\"   RMSE: {test_metrics['rmse']:.4f}\")\n",
    "print(f\"   MAE:  {test_metrics['mae']:.4f}\")\n",
    "print(f\"   MAPE: {test_metrics['mape']:.2f}%\")\n",
    "\n",
    "print(\"\\nüìà Model Architecture:\")\n",
    "print(f\"   Input Features: {input_size}\")\n",
    "print(f\"   Hidden Units: {hidden_size}\")\n",
    "print(f\"   LSTM Layers: {num_layers}\")\n",
    "print(f\"   Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\nüíæ Model Artifacts:\")\n",
    "print(f\"   MLflow Run ID: {run.info.run_id}\")\n",
    "print(\"   Model Registry: lstm-time-series-forecasting\")\n",
    "\n",
    "print(\"\\nüîß Next Steps:\")\n",
    "print(\"   1. Deploy model using Azure ML endpoints\")\n",
    "print(\"   2. Implement automated retraining pipelines\")\n",
    "print(\"   3. Add model monitoring and drift detection\")\n",
    "print(\"   4. Experiment with advanced architectures (Transformer, GRU)\")\n",
    "\n",
    "print(\"\\nüåê MLflow UI:\")\n",
    "print(\"   Run: mlflow ui --backend-store-uri ./mlruns\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_env",
   "language": "python",
   "name": "aml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
